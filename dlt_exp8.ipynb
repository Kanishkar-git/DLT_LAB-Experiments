{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtfRGvflpRpPuMZiYXkDHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanishkar-git/DLT_LAB-Experiments/blob/master/dlt_exp8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI_O0WnZ6R68",
        "outputId": "612168b0-54e5-403e-a412-af0291087af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : how are you\n",
            "Bot : <start> <start> <start> <start> <start> <start> <start>\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Step 1: Sample conversation data\n",
        "input_texts = ['hello', 'how are you', 'what is your name', 'bye']\n",
        "target_texts = ['hi', 'i am fine', 'i am a bot', 'goodbye']\n",
        "\n",
        "# Add start and end tokens\n",
        "target_texts = ['<start> ' + txt + ' <end>' for txt in target_texts]\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "input_sequences = pad_sequences(input_sequences, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_input_len = input_sequences.shape[1]\n",
        "max_target_len = target_sequences.shape[1]\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(\n",
        "    lstm_units, return_sequences=True, return_state=True\n",
        ")(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm_outputs, _, _ = LSTM(\n",
        "    lstm_units, return_sequences=True, return_state=True\n",
        ")(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention and context\n",
        "attention_layer = Attention()\n",
        "context_vector = attention_layer([decoder_lstm_outputs, encoder_outputs])\n",
        "combined = Concatenate(axis=-1)([context_vector, decoder_lstm_outputs])\n",
        "decoder_outputs = Dense(vocab_size, activation='softmax')(combined)\n",
        "\n",
        "# Final model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Prepare target output\n",
        "decoder_target_data = np.expand_dims(target_sequences, -1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    [input_sequences, target_sequences],\n",
        "    decoder_target_data,\n",
        "    batch_size=2,\n",
        "    epochs=200,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# ============ Inference Setup ============ #\n",
        "\n",
        "# Encoder model for inference\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "dec_state_input_h = Input(shape=(lstm_units,))\n",
        "dec_state_input_c = Input(shape=(lstm_units,))\n",
        "enc_output_input = Input(shape=(max_input_len, lstm_units))\n",
        "\n",
        "dec_emb2 = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "dec_lstm_outputs2, state_h2, state_c2 = LSTM(\n",
        "    lstm_units, return_sequences=True, return_state=True\n",
        ")(dec_emb2, initial_state=[dec_state_input_h, dec_state_input_c])\n",
        "\n",
        "context_vector2 = Attention()([dec_lstm_outputs2, enc_output_input])\n",
        "concat2 = Concatenate(axis=-1)([context_vector2, dec_lstm_outputs2])\n",
        "dec_outputs2 = Dense(vocab_size, activation='softmax')(concat2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, enc_output_input, dec_state_input_h, dec_state_input_c],\n",
        "    [dec_outputs2, state_h2, state_c2]\n",
        ")\n",
        "\n",
        "# ============ Response Generator ============ #\n",
        "\n",
        "def generate_response(input_text):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
        "    enc_out, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    target_seq = np.array([[tokenizer.word_index['<start>']]])\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq, enc_out, state_h, state_c], verbose=0\n",
        "        )\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if (\n",
        "            sampled_word == '<end>'\n",
        "            or sampled_word == ''\n",
        "            or len(decoded_sentence.split()) > max_target_len\n",
        "        ):\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += sampled_word + ' '\n",
        "\n",
        "            target_seq = np.array([[sampled_token_index]])\n",
        "            state_h, state_c = h, c\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# ============ Test ============ #\n",
        "\n",
        "test_input = \"how are you\"\n",
        "print(\"Input :\", test_input)\n",
        "print(\"Bot :\", generate_response(test_input))\n"
      ]
    }
  ]
}